{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1377609,"sourceType":"datasetVersion","datasetId":630055,"isSourceIdPinned":false}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nfrom google.colab import files\n\n# Download latest version\npath = kagglehub.dataset_download(\"rohanrao/air-quality-data-in-india\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pmdarima","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Library imports and setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pmdarima import auto_arima\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configure plot settings\nplt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = (12, 6)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data loading and preprocessing\ndef load_and_preprocess_data():\n    # Load data\n    data = pd.read_csv('/kaggle/input/air-quality-data-in-india/city_hour.csv', \n                      parse_dates=['Datetime'], \n                      index_col='Datetime')\n    \n    # Select data for Delhi and PM2.5\n    city_data = data[data['City'] == 'Delhi'][['PM2.5']].dropna()\n    \n    # Convert to daily data\n    daily_data = city_data.resample('D').mean().dropna()\n    \n    return daily_data\n\n# Call data loading function\nts_data = load_and_preprocess_data()\n\n# Data inspection\nprint(\"First 5 entries:\")\nprint(ts_data.head())\nprint(\"\\nStatistical description:\")\nprint(ts_data.describe())\nprint(\"\\nNumber of missing values:\", ts_data.isna().sum())\n\n# Time series visualization\nplt.figure(figsize=(14, 6))\nplt.plot(ts_data, label='PM2.5 (Delhi, daily)')\nplt.title('PM2.5 Time Series for Delhi')\nplt.xlabel('Date')\nplt.ylabel('PM2.5 Concentration')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data cleaning and anomaly removal\ndef clean_data(data):\n    # Anomaly detection\n    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n    anomalies = iso_forest.fit_predict(data.values.reshape(-1, 1))\n    \n    # Remove anomalies\n    cleaned_data = data[anomalies == 1]\n    \n    # Fill missing values\n    cleaned_data = cleaned_data.reindex(data.index)\n    cleaned_data = cleaned_data.interpolate(method='time').fillna(method='ffill').fillna(method='bfill')\n    \n    return cleaned_data\n\n# Clean the data\ncleaned_ts = clean_data(ts_data)\n\n# Verify cleaning results\nprint(\"\\nData size before cleaning:\", len(ts_data))\nprint(\"Data size after cleaning:\", len(cleaned_ts))\nprint(\"Missing values after cleaning:\", cleaned_ts.isna().sum())\n\n# Visualize cleaned data\nplt.figure(figsize=(14, 6))\nplt.plot(cleaned_ts, label='Cleaned PM2.5 Series', color='green')\nplt.title('Time Series after Cleaning')\nplt.xlabel('Date')\nplt.ylabel('PM2.5 Concentration')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Time series analysis\ndef analyze_time_series(data):\n    # Stationarity test\n    result = adfuller(data)\n    print(\"\\nAugmented Dickey-Fuller Test Results:\")\n    print(f\"p-value: {result[1]:.4f}\")\n    if result[1] > 0.05:\n        print(\"Series is non-stationary - differencing is required\")\n    else:\n        print(\"Series is stationary\")\n    \n    # Series decomposition\n    decomposition = seasonal_decompose(data, model='additive', period=30)\n    \n    # Visualize decomposition\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 10))\n    decomposition.observed.plot(ax=ax1, title='Original Series')\n    decomposition.trend.plot(ax=ax2, title='Trend')\n    decomposition.seasonal.plot(ax=ax3, title='Seasonality')\n    decomposition.resid.plot(ax=ax4, title='Residuals')\n    plt.tight_layout()\n    plt.show()\n\n# Perform analysis\nanalyze_time_series(cleaned_ts['PM2.5'])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting into training and test sets\ntrain_size = int(len(cleaned_ts) * 0.8)\ntrain = cleaned_ts.iloc[:train_size]\ntest = cleaned_ts.iloc[train_size:]\n\nprint(f\"\\nTraining set size: {len(train)}\")\nprint(f\"Test set size: {len(test)}\")\n\n# Visualize the split\nplt.figure(figsize=(14, 6))\nplt.plot(train, label='Training Data')\nplt.plot(test, label='Test Data')\nplt.title('Training and Test Set Split')\nplt.xlabel('Date')\nplt.ylabel('PM2.5 Concentration')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Forecasting using ARIMA\ndef arima_forecast(train_data, test_data):\n    try:\n        print(\"\\nFitting ARIMA model...\")\n        model = auto_arima(train_data, seasonal=True, m=12,\n                           suppress_warnings=True,\n                           stepwise=True, trace=True)\n        \n        print(\"\\nObtained model parameters:\")\n        print(model.summary())\n        \n        # Forecast\n        forecast = model.predict(n_periods=len(test_data))\n        forecast = pd.Series(forecast, index=test_data.index)\n        \n        # Fill any potential NaN values\n        forecast = forecast.fillna(method='ffill').fillna(method='bfill')\n        \n        return forecast\n    \n    except Exception as e:\n        print(f\"\\nError building ARIMA model: {str(e)}\")\n        return None\n\n\n# Perform ARIMA forecast\narima_pred = arima_forecast(train['PM2.5'], test['PM2.5'])\n\n# Visualize ARIMA results\nif arima_pred is not None:\n    plt.figure(figsize=(14, 6))\n    plt.plot(train['PM2.5'], label='Training Data')\n    plt.plot(test['PM2.5'], label='Actual Values')\n    plt.plot(arima_pred, label='ARIMA Forecast', linestyle='--')\n    plt.title('Forecasting using ARIMA')\n    plt.xlabel('Date')\n    plt.ylabel('PM2.5 Concentration')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Forecasting using Holt-Winters\ndef holt_winters_forecast(train_data, test_data):\n    try:\n        print(\"\\nTraining Holt-Winters model...\")\n        model = ExponentialSmoothing(train_data,\n                                     seasonal='add',\n                                     trend='add',\n                                     seasonal_periods=30)\n        model_fit = model.fit()\n        \n        # Forecast\n        forecast = model_fit.forecast(len(test_data))\n        forecast = pd.Series(forecast, index=test_data.index)\n        \n        return forecast\n    \n    except Exception as e:\n        print(f\"\\nError building Holt-Winters model: {str(e)}\")\n        return None\n\n# Perform Holt-Winters forecast\nhw_pred = holt_winters_forecast(train['PM2.5'], test['PM2.5'])\n\n# Visualize Holt-Winters results\nif hw_pred is not None:\n    plt.figure(figsize=(14, 6))\n    plt.plot(train['PM2.5'], label='Training Data')\n    plt.plot(test['PM2.5'], label='Actual Values')\n    plt.plot(hw_pred, label='Holt-Winters Forecast', linestyle='--')\n    plt.title('Forecasting using Holt-Winters')\n    plt.xlabel('Date')\n    plt.ylabel('PM2.5 Concentration')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Forecasting using LSTM\ndef lstm_forecast(train_data, test_data, n_steps=30, epochs=50):\n    try:\n        print(\"\\nPreparing data for LSTM...\")\n        # Normalize data\n        scaler = MinMaxScaler()\n        train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\n        test_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n        \n        # Create sequences\n        def create_sequences(data, seq_length):\n            X, y = [], []\n            for i in range(len(data)-seq_length):\n                X.append(data[i:i+seq_length])\n                y.append(data[i+seq_length])\n            return np.array(X), np.array(y)\n        \n        X_train, y_train = create_sequences(train_scaled, n_steps)\n        X_test, y_test = create_sequences(test_scaled, n_steps)\n         \n        # Build model\n        model = Sequential([\n            LSTM(50, activation='relu', input_shape=(n_steps, 1)),\n            Dense(1)\n        ])\n        model.compile(optimizer='adam', loss='mse')\n        \n        # Train model\n        print(\"\\nTraining LSTM model...\")\n        history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=0)\n        \n        # Forecast\n        forecast_scaled = model.predict(X_test, verbose=0)\n        forecast = scaler.inverse_transform(forecast_scaled)\n        \n        # Create forecast time series\n        forecast_series = pd.Series(forecast.flatten(), \n                                    index=test_data.index[n_steps:])\n        \n        return forecast_series\n         \n    except Exception as e:\n        print(f\"\\nError building LSTM model: {str(e)}\")\n        return None\n\n# Perform LSTM forecast\nlstm_pred = lstm_forecast(train['PM2.5'], test['PM2.5'])\n\n# Visualize LSTM results\nif lstm_pred is not None:\n    plt.figure(figsize=(14, 6))\n    plt.plot(train['PM2.5'], label='Training Data')\n    plt.plot(test['PM2.5'], label='Actual Values')\n    plt.plot(lstm_pred, label='LSTM Forecast', linestyle='--')\n    plt.title('Forecasting using LSTM')\n    plt.xlabel('Date')\n    plt.ylabel('PM2.5 Concentration')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Forecast quality evaluation\ndef evaluate_forecast(actual, predicted, model_name):\n    # Create DataFrame for easier processing\n    eval_df = pd.DataFrame({\n        'actual': actual,\n        'predicted': predicted\n    }).dropna()\n    \n    if len(eval_df) == 0:\n        print(f\"\\n{model_name}: No data for evaluation\")\n        return\n    \n    # Calculate metrics\n    mae = mean_absolute_error(eval_df['actual'], eval_df['predicted'])\n    rmse = np.sqrt(mean_squared_error(eval_df['actual'], eval_df['predicted']))\n    \n    try:\n        mape = np.mean(np.abs((eval_df['actual'] - eval_df['predicted']) / eval_df['actual'])) * 100\n    except:\n        mape = np.nan\n    \n    print(f\"\\nEvaluation results for {model_name}:\")\n    print(f\"MAE: {mae:.2f}\")\n    print(f\"RMSE: {rmse:.2f}\")\n    if not np.isnan(mape):\n        print(f\"MAPE: {mape:.2f}%\")\n    print(f\"Number of points for evaluation: {len(eval_df)}\")\n\n# Evaluate models\nif arima_pred is not None:\n    evaluate_forecast(test['PM2.5'], arima_pred, \"ARIMA\")\n\nif hw_pred is not None:\n    evaluate_forecast(test['PM2.5'], hw_pred, \"Holt-Winters\")\n\nif lstm_pred is not None:\n    evaluate_forecast(test['PM2.5'][len(test)-len(lstm_pred):], lstm_pred, \"LSTM\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conclusions and recommendations\n\nprint(\" ARIMA usually performs well for stationary series\")\nprint(\" To improve results, try:\")\nprint(\"   - Additional data cleaning\")\nprint(\"   - Hyperparameter tuning for models\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}